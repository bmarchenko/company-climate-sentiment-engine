{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from boilerpipe.extract import Extractor\n",
    "from urllib.parse import urlparse\n",
    "from langdetect import detect\n",
    "LINKS_DIR = \"../1-scrape-google-links/scrape_result4\"\n",
    "OUT_FILE_NAME = \"extract.csv\"\n",
    "MIN_TEXT_LENGTH = 1300\n",
    "STOP_SITES = (\"twitter.com\", \"facebook.com\", \"youtube.com\", \"wikipedia\", \"slideshare.net\", \".pdf\", \"slideplayer.com\", \"cdp.net\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextExtractor():\n",
    "    def __init__(self, company, df, spamwriter, *args, **kwargs):\n",
    "        self.company = company\n",
    "        self.spamwriter = spamwriter\n",
    "        self.df = df\n",
    "    \n",
    "    def is_url_ok(self, url):\n",
    "        parsed_url = urlparse(url)\n",
    "        if any(word in url for word in STOP_SITES):\n",
    "            print(\"Skipping due to stop website: {}\".format(url))\n",
    "            return False\n",
    "        elif company.lower() in parsed_url.netloc:\n",
    "            print(\"Skipping due to company's website: {} in {}\".format(self.company, url))\n",
    "            return False\n",
    "        elif parsed_url.path in [\"\", \"/\", '/en/', '/en']:\n",
    "            print(\"Skipping due to front page {}\".format(url))\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def is_text_ok(self, text):\n",
    "        if detect(text) != \"en\":\n",
    "            print(\"Skipping: Extracted text not English: {}\".format(text))\n",
    "            return False\n",
    "        elif len(text) < MIN_TEXT_LENGTH:\n",
    "            print(\"Skipping: Extracted text is too short: {}\".format(text))\n",
    "            return False\n",
    "#         elif self.company not in text:\n",
    "#             print(\"Skipping: No company name in Extracted text: {}\".format(self.company))\n",
    "#             return False\n",
    "        return True\n",
    "\n",
    "    def highlight_kewords(self, text):\n",
    "        return re.sub('(climate policy|climate change|innovation|climate|pollution|sustainable|devastation|CO2|CO\\(2\\)|carbon|{})'.format(self.company), r'<mark>\\1</mark>', text, flags=re.IGNORECASE)\n",
    "\n",
    "    def extract_file_data(self):\n",
    "        data = []\n",
    "        for index, row in self.df.iterrows():\n",
    "            url = row[\"url\"]\n",
    "            if not self.is_url_ok(url):\n",
    "                continue\n",
    "            print(url)\n",
    "            try:\n",
    "                extractor = Extractor(extractor='ArticleExtractor', url=url)\n",
    "                extracted_html = extractor.getHTML()\n",
    "                if self.is_text_ok(extracted_text):\n",
    "                    spamwriter.writerow((company, url, row.get(\"title\", \"\"), row.get(\"text\", \"\"), self.highlight_kewords(extracted_text)))\n",
    "            except Exception as ex:\n",
    "                print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chevron\n",
      "Extracting Chevron\n",
      "Skipping due to company's website: Chevron in https://australia.chevron.com/environment/protecting-the-environment\n",
      "https://www.eenews.net/stories/1060077139\n",
      "https://www.sciencefriday.com/segments/climate-science-goes-to-court-in-california-oil-case/\n",
      "Skipping due to stop website: http://www1.nyc.gov/assets/home/downloads/pdf/press-releases/2018/complaint-filed-8031957-20180109.pdf\n",
      "http://blogs.edf.org/energyexchange/2017/08/25/heres-how-chevrons-next-ceo-can-turn-over-a-new-leaf/\n",
      "http://www.bdlaw.com/news-2211.html\n",
      "https://grist.org/briefly/exxonmobil-and-chevron-are-some-of-the-most-influential-climate-lobbyists-yikes/\n",
      "Remote end closed connection without response\n",
      "http://www.chicagotribune.com/business/sns-bc-us--california-climate-change-lawsuits-20180321-story.html\n",
      "http://fortune.com/2016/05/24/exxonmobil-chevron-shareholder-meetings-climate/\n",
      "https://www.insurancejournal.com/news/national/2018/03/26/484378.htm\n"
     ]
    }
   ],
   "source": [
    "links_number = 0\n",
    "with open(OUT_FILE_NAME, \"w\") as f:\n",
    "    spamwriter = csv.writer(f)\n",
    "    spamwriter.writerow([\"company\", \"url\", \"title\", \"extract\", \"content\"])\n",
    "    for company in os.listdir(LINKS_DIR):\n",
    "        if company == \"Chevron\":\n",
    "            print(company)\n",
    "            if os.path.isfile(LINKS_DIR+\"/\"+company):\n",
    "                continue\n",
    "            print(\"Extracting {}\".format(company))\n",
    "            fl = os.listdir(\"{}/{}\".format(LINKS_DIR, company))[1]\n",
    "            if fl.endswith(\".csv\"):\n",
    "                df = pd.read_csv(\"{}/{}/{}\".format(LINKS_DIR, company, fl))\n",
    "                links_number += len(df)\n",
    "                TextExtractor(company, df, spamwriter).extract_file_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY: Extracted texts of 9 articles out of 10 links\n",
      "MOST POPULAR SOURCES: netloc\n",
      "www.vice.com            1\n",
      "www.sec.gov             1\n",
      "www.eastbaytimes.com    1\n",
      "www.apnews.com          1\n",
      "thehill.com             1\n",
      "money.cnn.com           1\n",
      "inhabitat.com           1\n",
      "freebeacon.com          1\n",
      "earther.com             1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(OUT_FILE_NAME)\n",
    "print(\"SUMMARY: Extracted texts of {} articles out of {} links\".format(len(df), links_number))\n",
    "df[\"netloc\"] =  df.apply(lambda row: urlparse(row.url).netloc,axis=1)\n",
    "print(\"MOST POPULAR SOURCES: {}\".format(df.groupby(\"netloc\").size().sort_values(ascending=False, inplace=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(OUT_FILE_NAME)\n",
    "df2 = df.head(7)\n",
    "df2.to_csv('chevron_short.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "sentiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
