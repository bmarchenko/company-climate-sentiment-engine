{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from boilerpipe.extract import Extractor\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words())\n",
    "INPUT_FILE = \"../1-Data/2-extract-text/extracted_texts/Intel.csv\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "alternative_company_names = {\"AMD (Advanced Micro Devices)\": \"AMD\",\n",
    "                    'Royal Dutch Shell PLC': \"Shell\",\n",
    "                    \"Samsung Electronics Co., Ltd.\": \"Samsung\",\n",
    "                    \"Goodyear Tire & Rubber Co\": \"Goodyear\",\n",
    "                    \"Sumitomo Rubber Industries\": \"Sumitomo\",\n",
    "                    \"Exxon Mobil Corp.\": \"ExxonMobil\",\n",
    "                    \"General Motors Corp.\": \"GM\",\n",
    "                    \"Ford Motor Co.\": \"Ford\",\n",
    "                    \"Toyota Motor Corp.\": \"Toyota\",\n",
    "                    \"Petro China\": \"PetroChina\",\n",
    "                    'Volkswagen AG': \"VW\"}\n",
    "\n",
    "def clean_text(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\") # create a new bs4 object from the html data loaded\n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "    texts = soup.findAll(text=True)\n",
    "#     import ipdb; ipdb.set_trace()\n",
    "#     print(len(texts))\n",
    "    text = \". \".join(t.strip() for t in texts)\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text from html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_FILE)\n",
    "df['text'] = df.apply(lambda row: \"{} {}\".format(row['title'], clean_text(str(row['content']))), axis=1)\n",
    "df.drop(df[df.text.str.len() < 150].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPANY_NAMES_STOP_WORDS = \"PLC|Corp\"\n",
    "with open(\"company-suffix.txt\", \"r\") as fl:\n",
    "    text = [i for i in fl.read().split('\\n') if not i.startswith('//')]\n",
    "    COMPANY_NAMES_STOP_WORDS = \"(\" + \"$)|(\".join(text) + \"$)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_names(company):\n",
    "    company_names = [re.sub(COMPANY_NAMES_STOP_WORDS, '', company, flags=re.IGNORECASE).strip().lower()]\n",
    "    if company in alternative_company_names:\n",
    "        company_names.append(alternative_company_names[company].lower())\n",
    "    #Company is often mentioned by part of it's name. e.g. \"Royal Dutch Shell\" -> \"Shell\"\n",
    "#     company_names = set([company] + [i for i in company.split() if len(i)>2])\n",
    "    return company_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Again  \n",
    "\n",
    "taken from https://gist.github.com/bbengfort/044682e76def583a12e6c09209c664a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split as tts\n",
    "\n",
    "\n",
    "def timeit(func):\n",
    "    \"\"\"\n",
    "    Simple timing decorator\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start  = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        delta  = time.time() - start\n",
    "        return result, delta\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n",
    "\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transforms input data by using NLTK tokenization, lemmatization, and\n",
    "    other normalization and filtering techniques.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None, lower=True, strip=True):\n",
    "        \"\"\"\n",
    "        Instantiates the preprocessor, which make load corpora, models, or do\n",
    "        other time-intenstive NLTK data loading.\n",
    "        \"\"\"\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = set(stopwords) if stopwords else set(sw.words('english'))\n",
    "        self.punct      = set(punct) if punct else set(string.punctuation)\n",
    "        self.punct.add(\"â€œ\")\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit simply returns self, no other information is needed.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"\n",
    "        No inverse transformation\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Actually runs the preprocessing on each document.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        \"\"\"\n",
    "        Returns a normalized, lemmatized list of tokens from a document by\n",
    "        applying segmentation (breaking into sentences), then word/punctuation\n",
    "        tokenization, and finally part of speech tagging. It uses the part of\n",
    "        speech tags to look up the lemma in WordNet, and returns the lowercase\n",
    "        version of all the words, removing stopwords and punctuation.\n",
    "        \"\"\"\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If punctuation or stopword, ignore token and continue\n",
    "                if token in self.stopwords or all(char in self.punct for char in token):\n",
    "                    continue\n",
    "                if tag == \"NNP\":\n",
    "                    continue\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        \"\"\"\n",
    "        Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n",
    "        tag to perform much more accurate WordNet lemmatization.\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "\n",
    "\n",
    "@timeit\n",
    "def build_and_evaluate(X, y, classifier=SGDClassifier, outpath=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Builds a classifer for the given list of documents and targets in two\n",
    "    stages: the first does a train/test split and prints a classifier report,\n",
    "    the second rebuilds the model on the entire corpus and returns it for\n",
    "    operationalization.\n",
    "    X: a list or iterable of raw strings, each representing a document.\n",
    "    y: a list or iterable of labels, which will be label encoded.\n",
    "    Can specify the classifier to build with: if a class is specified then\n",
    "    this will build the model with the Scikit-Learn defaults, if an instance\n",
    "    is given, then it will be used directly in the build pipeline.\n",
    "    If outpath is given, this function will write the model as a pickle.\n",
    "    If verbose, this function will print out information to the command line.\n",
    "    \"\"\"\n",
    "\n",
    "    @timeit\n",
    "    def build(classifier, X, y=None):\n",
    "        \"\"\"\n",
    "        Inner build function that builds a single model.\n",
    "        \"\"\"\n",
    "        if isinstance(classifier, type):\n",
    "            classifier = classifier(loss='log')\n",
    "\n",
    "        model = Pipeline([\n",
    "            ('preprocessor', NLTKPreprocessor()),\n",
    "            ('vectorizer', TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False)),\n",
    "            ('classifier', classifier),\n",
    "        ])\n",
    "\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "    # Label encode the targets\n",
    "    labels = LabelEncoder()\n",
    "    y = labels.fit_transform(y)\n",
    "\n",
    "    # Begin evaluation\n",
    "    if verbose: print(\"Building for evaluation\")\n",
    "    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2)\n",
    "    model, secs = build(classifier, X_train, y_train)\n",
    "\n",
    "    if verbose: print(\"Evaluation model fit in {:0.3f} seconds\".format(secs))\n",
    "    if verbose: print(\"Classification Report:\\n\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(clsr(y_test, y_pred, target_names=labels.classes_))\n",
    "\n",
    "    if verbose: print(\"Building complete model and saving ...\")\n",
    "    model, secs = build(classifier, X, y)\n",
    "    model.labels_ = labels\n",
    "\n",
    "    if verbose: print(\"Complete model fit in {:0.3f} seconds\".format(secs))\n",
    "\n",
    "    if outpath:\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        print(\"Model written out to {}\".format(outpath))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def show_most_informative_features(model, text=None, n=20):\n",
    "    \"\"\"\n",
    "    Accepts a Pipeline with a classifer and a TfidfVectorizer and computes\n",
    "    the n most informative features of the model. If text is given, then will\n",
    "    compute the most informative features for classifying that text.\n",
    "    Note that this function will only work on linear models with coefs_\n",
    "    \"\"\"\n",
    "    # Extract the vectorizer and the classifier from the pipeline\n",
    "    vectorizer = model.named_steps['vectorizer']\n",
    "    classifier = model.named_steps['classifier']\n",
    "\n",
    "    # Check to make sure that we can perform this computation\n",
    "    if not hasattr(classifier, 'coef_'):\n",
    "        raise TypeError(\n",
    "            \"Cannot compute most informative features on {} model.\".format(\n",
    "                classifier.__class__.__name__\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if text is not None:\n",
    "        # Compute the coefficients for the text\n",
    "        tvec = model.transform([text]).toarray()\n",
    "    else:\n",
    "        # Otherwise simply use the coefficients\n",
    "        tvec = classifier.coef_\n",
    "\n",
    "    # Zip the feature names with the coefs and sort\n",
    "    coefs = sorted(\n",
    "        zip(tvec[0], vectorizer.get_feature_names()),\n",
    "        key=itemgetter(0), reverse=False\n",
    "    )\n",
    "\n",
    "    topn  = zip(coefs[:n], coefs[:-(n+1):-1])\n",
    "\n",
    "    # Create the output string to return\n",
    "    output = []\n",
    "\n",
    "    # If text, add the predicted value to the output.\n",
    "    if text is not None:\n",
    "        output.append(\"\\\"{}\\\"\".format(text))\n",
    "        output.append(\"Classified as: {}\".format(model.predict([text])))\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Create two columns with most negative and most positive features.\n",
    "    for (cp, fnp), (cn, fnn) in topn:\n",
    "        output.append(\n",
    "            \"{:0.4f}{: >15}    {:0.4f}{: >15}\".format(cp, fnp, cn, fnn)\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"model.pickle\", \"rb\") as fl:\n",
    "    model = pickle.load(fl)\n",
    "df.columns\n",
    "df['text'] = df.apply(lambda row: \"{} {}\".format(row['title'], clean_text(str(row['content']))), axis=1)\n",
    "df.drop(df[df.text.str.len() < 150].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel: p: 15, n: 5,  'pos_p': 13.201965950358762, 'neg_p': 4.899903099740847, neg_url: https://mashable.com/2017/12/18/trump-national-security-strategy-omits-climate-change-threat/,  pos_url: https://www.greenbiz.com/article/alphabet-intel-and-walmart-low-carbon-products-make-business-sense,\n"
     ]
    }
   ],
   "source": [
    "companies = df.groupby(\"company\", as_index=False)\n",
    "rows_list = []\n",
    "for num, cp in enumerate(list(companies.groups.keys())):\n",
    "    neg_url = ''\n",
    "    pos_url = ''\n",
    "    cp_df = df.loc[df['company'] == cp]\n",
    "#     cp_df['lemmatized'] = cp_df.apply(lambda row: \" \".join(tokenize(row['text'], get_company_names(row[\"\"]))), axis=1)\n",
    "    urls = list(cp_df['url'])\n",
    "#     cp_df['text'] = cp_df.apply(lambda row: \"{} {}\".format(row[''], clean_text(str(row['content']))), axis=1)\n",
    "    \n",
    "    res = model.predict(cp_df['text'])\n",
    "    counter = Counter(res)\n",
    "    probs = model.predict_proba(cp_df['text'])\n",
    "    \n",
    "    negs, pos, neut = zip(*probs)\n",
    "    neg_url = urls[negs.index(max(negs))]\n",
    "    pos_url = urls[pos.index(max(pos))]\n",
    "\n",
    "    rows_list.append({\"company\": cp, \"pos_c\":Counter(res)[1], \"neg_c\": Counter(res)[0], 'pos_p': sum(pos), 'neg_p': sum(negs), 'neg_url': f\"<a href='{neg_url}' target='_blank'>Clickme</a>\", 'pos_url': f\"<a href='{pos_url}' target='_blank'>Clickme</a>\"})#, \"neg_url\": neg_url,\"pos_url\": pos_url })\n",
    "    print(f\"{cp}: p: {Counter(res)[1]}, n: {Counter(res)[0]},  'pos_p': {sum(pos)}, 'neg_p': {sum(negs)}, neg_url: {neg_url},  pos_url: {pos_url},\")\n",
    "    \n",
    "final_df =  pd.DataFrame(rows_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\"Food\": [\"Cargill Inc\", \"Nestle SA\", \"Procter & Gamble, Co.\", \"PepsiCo Inc.\", \"Bunge\"],\n",
    "            \"Chemicals\": [\"BASF AG\", \"Johnson & Johnson\", \"Bayer AG\", \"Ineos\", \"Dow Chemical\"],\n",
    "            \"Electronics\": [\"Samsung Electronics Co., Ltd.\", \"Sony Corp.\", \"AMD (Advanced Micro Devices)\", \"Toshiba Corp.\", \"Motorola Inc.\"],\n",
    "            \"Metals & mining\": [\"Glencore International AG\", \"ArcelorMittal\", \"Agnico-Eagle Mines Ltd.\", \"ThyssenKrupp Stahl\", \"Arcelor SA\"],\n",
    "            \"Tires\": [\"Michelin\", \"Bridgestone Corp.\", \"Continental AG\", \"Goodyear Tire & Rubber Co\", \"Sumitomo Rubber Industries\"],\n",
    "            \"Energy & water\": [\"GazProm\", \"Duke Energy\", \"Petrobras\", \"Petro China\", \"RosNeft\", \"BP plc\", \"Exxon Mobil Corp.\", \"Total SA\", 'Royal Dutch Shell PLC', \"Chevron Corp.\"],\n",
    "            \"Automotive & transport\": [\"General Motors Corp.\", \"BMW\", \"ABB\", \"Daimler AG\", \"Ford Motor Co.\", \"Toyota Motor Corp.\", \"Volkswagen AG\"],\n",
    "            \"Wood & paper products\": [\"International Paper Corp.\", \"Georgia Pacific\", \"Weyerhaeuser Co.\", \"Stora Enso Oyj\", \"Kimberly-Clark Corp.\"],\n",
    "            \"Computer services & software\": [\"IBM\", \"Microsoft Corp.\", \"EDS Corp.\", \"AuFeminin.com\", \"Amazon.com Inc.\", \"Apple Inc\"],\n",
    "#             \"Dirty Dozen\": [\"GazProm\", \"Petrobras\", \"RosNeft\", \"Petro China\", \"Duke Energy\"]             \n",
    "#             \"Custom\": [\"\"]\n",
    "}\n",
    "companies_dc = {}\n",
    "for cat, companies in categories.items():\n",
    "    for c in companies:\n",
    "        companies_dc[c] = cat\n",
    "cats = list(categories.keys())\n",
    "final_df['category'] = final_df.apply(lambda row: companies_dc[row['company']], axis=1)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "cmap = get_cmap(len(cats))\n",
    "#http://www.climatechangenews.com/2015/05/21/the-dirty-dozen-the-fossil-fuel-industrys-polluting-league-table/\n",
    "dirty_dozen = [\"GazProm\", \"Petrobras\", \"RosNeft\", \"Petro China\", \"Glencore International AG\", 'Royal Dutch Shell PLC', \"Exxon Mobil Corp.\", \"BP plc\", \"Chevron Corp.\"]\n",
    "\n",
    "def highlight_categories(row):\n",
    "    return [f'background-color: {colors.rgb2hex(cmap(cats.index(row[\"category\"])))}' for i in row]\n",
    "\n",
    "def highlight_dirty_dozen(row):\n",
    "    if row[\"company\"] in dirty_dozen:\n",
    "        return ['background-color: red' for i in row]\n",
    "    else:\n",
    "        return [\"\" for i in row]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by absolute number of negatives/positives.  Highlight sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col0 {\n",
       "            background-color:  #ff0018;\n",
       "        }    #T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col1 {\n",
       "            background-color:  #ff0018;\n",
       "        }    #T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col2 {\n",
       "            background-color:  #ff0018;\n",
       "        }    #T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col3 {\n",
       "            background-color:  #ff0018;\n",
       "        }    #T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col4 {\n",
       "            background-color:  #ff0018;\n",
       "        }    #T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col5 {\n",
       "            background-color:  #ff0018;\n",
       "        }    #T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col6 {\n",
       "            background-color:  #ff0018;\n",
       "        }    #T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col7 {\n",
       "            background-color:  #ff0018;\n",
       "        }    #T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col8 {\n",
       "            background-color:  #ff0018;\n",
       "        }</style>  \n",
       "<table id=\"T_fc99e070_64ec_11e8_958c_b8e8560d5912\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >company</th> \n",
       "        <th class=\"col_heading level0 col1\" >neg_c</th> \n",
       "        <th class=\"col_heading level0 col2\" >neg_p</th> \n",
       "        <th class=\"col_heading level0 col3\" >neg_url</th> \n",
       "        <th class=\"col_heading level0 col4\" >pos_c</th> \n",
       "        <th class=\"col_heading level0 col5\" >pos_p</th> \n",
       "        <th class=\"col_heading level0 col6\" >pos_url</th> \n",
       "        <th class=\"col_heading level0 col7\" >category</th> \n",
       "        <th class=\"col_heading level0 col8\" >mark</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_fc99e070_64ec_11e8_958c_b8e8560d5912level0_row0\" class=\"row_heading level0 row0\" >0</th> \n",
       "        <td id=\"T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col0\" class=\"data row0 col0\" >Intel</td> \n",
       "        <td id=\"T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col1\" class=\"data row0 col1\" >5</td> \n",
       "        <td id=\"T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col2\" class=\"data row0 col2\" >4.8999</td> \n",
       "        <td id=\"T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col3\" class=\"data row0 col3\" ><a href='https://mashable.com/2017/12/18/trump-national-security-strategy-omits-climate-change-threat/' target='_blank'>Clickme</a></td> \n",
       "        <td id=\"T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col4\" class=\"data row0 col4\" >15</td> \n",
       "        <td id=\"T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col5\" class=\"data row0 col5\" >13.202</td> \n",
       "        <td id=\"T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col6\" class=\"data row0 col6\" ><a href='https://www.greenbiz.com/article/alphabet-intel-and-walmart-low-carbon-products-make-business-sense' target='_blank'>Clickme</a></td> \n",
       "        <td id=\"T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col7\" class=\"data row0 col7\" >Custom</td> \n",
       "        <td id=\"T_fc99e070_64ec_11e8_958c_b8e8560d5912row0_col8\" class=\"data row0 col8\" >0.333333</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x128f1d2e8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['mark'] = final_df['neg_c']/final_df['pos_c']\n",
    "final_df = final_df.sort_values(['mark'], ascending=[0])\n",
    "final_df.style.apply(highlight_categories, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "sentiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
