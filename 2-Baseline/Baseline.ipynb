{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from boilerpipe.extract import Extractor\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words())\n",
    "ANNOTATION_RESULTS = \"../1-Data/3-annotation/output.csv\"\n",
    "df = pd.read_csv(ANNOTATION_RESULTS)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "alternative_company_names = {\"AMD (Advanced Micro Devices)\": \"AMD\",\n",
    "                    'Royal Dutch Shell PLC': \"Shell\",\n",
    "                    \"Samsung Electronics Co., Ltd.\": \"Samsung\",\n",
    "                    \"Goodyear Tire & Rubber Co\": \"Goodyear\",\n",
    "                    \"Sumitomo Rubber Industries\": \"Sumitomo\",\n",
    "                    \"Exxon Mobil Corp.\": \"Exxon\",\n",
    "                    \"General Motors Corp.\": \"GM\",\n",
    "                    \"Ford Motor Co.\": \"Ford\",\n",
    "                    \"Toyota Motor Corp.\": \"Toyota\",\n",
    "                    \"Petro China\": \"PetroChina\",\n",
    "                    'Volkswagen AG': \"VW\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text from html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\") # create a new bs4 object from the html data loaded\n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "    texts = soup.findAll(text=True)\n",
    "#     import ipdb; ipdb.set_trace()\n",
    "#     print(len(texts))\n",
    "    text = \". \".join(t.strip() for t in texts)\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    return text\n",
    "df['text'] = df.apply(lambda row: \"{} {}\".format(row['title'], clean_text(str(row['full_text']))), axis=1)\n",
    "df.drop(df[df.text.str.len() < 150].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPANY_NAMES_STOP_WORDS = \"PLC|Corp\"\n",
    "with open(\"company-suffix.txt\", \"r\") as fl:\n",
    "    text = [i for i in fl.read().split('\\n') if not i.startswith('//')]\n",
    "    COMPANY_NAMES_STOP_WORDS = \"(\" + \"$)|(\".join(text) + \"$)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_names(company):\n",
    "    company_names = [re.sub(COMPANY_NAMES_STOP_WORDS, '', company, flags=re.IGNORECASE).strip().lower()]\n",
    "    if company in alternative_company_names:\n",
    "        company_names.append(alternative_company_names[company].lower())\n",
    "    #Company is often mentioned by part of it's name. e.g. \"Royal Dutch Shell\" -> \"Shell\"\n",
    "#     company_names = set([company] + [i for i in company.split() if len(i)>2])\n",
    "    return company_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.max_colwidth = 10000\n",
    "# row = df.loc[df['url'] == 'http://www.sustainablebrands.com/solutionproviders/basf']\n",
    "# row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Baseline:  \n",
    "Using the same BoW technique as above  \n",
    "I exclude texts that are not related to the company or climate change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 215 texts\n",
      "Sentiment counts:\n",
      "Positive             0.362791\n",
      "Negative             0.232558\n",
      "Strongly Positive    0.181395\n",
      "Neutral              0.134884\n",
      "Strongly Negative    0.088372\n",
      "Name: Answer.sentiment, dtype: float64\n",
      "Simple Sentiment counts:\n",
      "1    0.544186\n",
      "0    0.320930\n",
      "2    0.134884\n",
      "Name: simple_sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target_names=('Strongly Negative', 'Negative', 'Neutral', 'Positive', 'Strongly Positive')\n",
    "cleaned_df = df[(df['Answer.CompanyConfidence'] != 0) & (df['Answer.ClimateConfidence'] != 0)]\n",
    "cleaned_df = cleaned_df.dropna(subset =[\"Answer.sentiment\"])\n",
    "\n",
    "# cleaned_df.dropna(subset=['Answer.sentiment'], how='all', inplace = True)\n",
    "simple_dc = {\"Strongly Negative\": '0', \"Negative\": '0', \"Neutral\": '2', \"Positive\": '1', \"Strongly Positive\": '1'}\n",
    "cleaned_df['simple_sentiment'] = cleaned_df.apply(lambda row: simple_dc[row['Answer.sentiment']] ,axis=1)\n",
    "print(\"Total: {} texts\".format(len(cleaned_df)))\n",
    "print(\"Sentiment counts:\")\n",
    "print(cleaned_df['Answer.sentiment'].value_counts(normalize=True))\n",
    "print(\"Simple Sentiment counts:\")\n",
    "print(cleaned_df['simple_sentiment'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Again  \n",
    "\n",
    "taken from https://gist.github.com/bbengfort/044682e76def583a12e6c09209c664a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split as tts\n",
    "\n",
    "\n",
    "def timeit(func):\n",
    "    \"\"\"\n",
    "    Simple timing decorator\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start  = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        delta  = time.time() - start\n",
    "        return result, delta\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n",
    "\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transforms input data by using NLTK tokenization, lemmatization, and\n",
    "    other normalization and filtering techniques.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None, lower=True, strip=True):\n",
    "        \"\"\"\n",
    "        Instantiates the preprocessor, which make load corpora, models, or do\n",
    "        other time-intenstive NLTK data loading.\n",
    "        \"\"\"\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = set(stopwords) if stopwords else set(sw.words('english'))\n",
    "        self.punct      = set(punct) if punct else set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit simply returns self, no other information is needed.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"\n",
    "        No inverse transformation\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Actually runs the preprocessing on each document.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        \"\"\"\n",
    "        Returns a normalized, lemmatized list of tokens from a document by\n",
    "        applying segmentation (breaking into sentences), then word/punctuation\n",
    "        tokenization, and finally part of speech tagging. It uses the part of\n",
    "        speech tags to look up the lemma in WordNet, and returns the lowercase\n",
    "        version of all the words, removing stopwords and punctuation.\n",
    "        \"\"\"\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If punctuation or stopword, ignore token and continue\n",
    "                if token in self.stopwords or all(char in self.punct for char in token):\n",
    "                    continue\n",
    "                if tag == \"NNP\":\n",
    "                    continue\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        \"\"\"\n",
    "        Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n",
    "        tag to perform much more accurate WordNet lemmatization.\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "\n",
    "\n",
    "@timeit\n",
    "def build_and_evaluate(X, y, classifier=SGDClassifier, outpath=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Builds a classifer for the given list of documents and targets in two\n",
    "    stages: the first does a train/test split and prints a classifier report,\n",
    "    the second rebuilds the model on the entire corpus and returns it for\n",
    "    operationalization.\n",
    "    X: a list or iterable of raw strings, each representing a document.\n",
    "    y: a list or iterable of labels, which will be label encoded.\n",
    "    Can specify the classifier to build with: if a class is specified then\n",
    "    this will build the model with the Scikit-Learn defaults, if an instance\n",
    "    is given, then it will be used directly in the build pipeline.\n",
    "    If outpath is given, this function will write the model as a pickle.\n",
    "    If verbose, this function will print out information to the command line.\n",
    "    \"\"\"\n",
    "\n",
    "    @timeit\n",
    "    def build(classifier, X, y=None):\n",
    "        \"\"\"\n",
    "        Inner build function that builds a single model.\n",
    "        \"\"\"\n",
    "        if isinstance(classifier, type):\n",
    "            classifier = classifier()\n",
    "\n",
    "        model = Pipeline([\n",
    "            ('preprocessor', NLTKPreprocessor()),\n",
    "            ('vectorizer', TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False)),\n",
    "            ('classifier', classifier),\n",
    "        ])\n",
    "\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "    # Label encode the targets\n",
    "    labels = LabelEncoder()\n",
    "    y = labels.fit_transform(y)\n",
    "\n",
    "    # Begin evaluation\n",
    "    if verbose: print(\"Building for evaluation\")\n",
    "    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2)\n",
    "    model, secs = build(classifier, X_train, y_train)\n",
    "\n",
    "    if verbose: print(\"Evaluation model fit in {:0.3f} seconds\".format(secs))\n",
    "    if verbose: print(\"Classification Report:\\n\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(clsr(y_test, y_pred, target_names=labels.classes_))\n",
    "\n",
    "    if verbose: print(\"Building complete model and saving ...\")\n",
    "    model, secs = build(classifier, X, y)\n",
    "    model.labels_ = labels\n",
    "\n",
    "    if verbose: print(\"Complete model fit in {:0.3f} seconds\".format(secs))\n",
    "\n",
    "    if outpath:\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        print(\"Model written out to {}\".format(outpath))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def show_most_informative_features(model, text=None, n=20):\n",
    "    \"\"\"\n",
    "    Accepts a Pipeline with a classifer and a TfidfVectorizer and computes\n",
    "    the n most informative features of the model. If text is given, then will\n",
    "    compute the most informative features for classifying that text.\n",
    "    Note that this function will only work on linear models with coefs_\n",
    "    \"\"\"\n",
    "    # Extract the vectorizer and the classifier from the pipeline\n",
    "    vectorizer = model.named_steps['vectorizer']\n",
    "    classifier = model.named_steps['classifier']\n",
    "\n",
    "    # Check to make sure that we can perform this computation\n",
    "    if not hasattr(classifier, 'coef_'):\n",
    "        raise TypeError(\n",
    "            \"Cannot compute most informative features on {} model.\".format(\n",
    "                classifier.__class__.__name__\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if text is not None:\n",
    "        # Compute the coefficients for the text\n",
    "        tvec = model.transform([text]).toarray()\n",
    "    else:\n",
    "        # Otherwise simply use the coefficients\n",
    "        tvec = classifier.coef_\n",
    "\n",
    "    # Zip the feature names with the coefs and sort\n",
    "    coefs = sorted(\n",
    "        zip(tvec[0], vectorizer.get_feature_names()),\n",
    "        key=itemgetter(0), reverse=False\n",
    "    )\n",
    "\n",
    "    topn  = zip(coefs[:n], coefs[:-(n+1):-1])\n",
    "\n",
    "    # Create the output string to return\n",
    "    output = []\n",
    "\n",
    "    # If text, add the predicted value to the output.\n",
    "    if text is not None:\n",
    "        output.append(\"\\\"{}\\\"\".format(text))\n",
    "        output.append(\"Classified as: {}\".format(model.predict([text])))\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Create two columns with most negative and most positive features.\n",
    "    for (cp, fnp), (cn, fnn) in topn:\n",
    "        output.append(\n",
    "            \"{:0.4f}{: >15}    {:0.4f}{: >15}\".format(cp, fnp, cn, fnn)\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(sw.words('english'))\n",
    "def lemmatize(token, tag):\n",
    "    \"\"\"\n",
    "    Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n",
    "    tag to perform much more accurate WordNet lemmatization.\n",
    "    \"\"\"\n",
    "    tag = {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def tokenize(document, company_names):\n",
    "    doc_tokens = []\n",
    "\n",
    "    for sent in sent_tokenize(document):\n",
    "        company_in_sent = False\n",
    "        sent_tokens = []\n",
    "        # Break the sentence into part of speech tagged tokens\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # Apply preprocessing to the token\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If punctuation or stopword, ignore token and continue\n",
    "#             import ipdb; ipdb.set_trace()\n",
    "            if token in stopwords or all(char in set(string.punctuation) for char in token):\n",
    "                continue\n",
    "            if tag == \"NNP\":\n",
    "                continue\n",
    "            # Lemmatize the token and yield\n",
    "            lemma = lemmatize(token, tag)\n",
    "            if lemma in company_names:\n",
    "                company_in_sent = True\n",
    "            sent_tokens.append(lemma)\n",
    "        if company_in_sent:\n",
    "            \n",
    "            doc_tokens.extend(sent_tokens)\n",
    "#                 break\n",
    "#     if len(doc_tokens) == 0:\n",
    "#         print(document)\n",
    "#         print(company_names)\n",
    "#         import ipdb; ipdb.set_trace()\n",
    "#     print(company_names[0], len(doc_tokens))\n",
    "    return doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['royal dutch shell', 'shell']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_company_names(list(cleaned_df[\"company\"])[1])\n",
    "# list(cleaned_df[\"company\"])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_company_names(list(cleaned_df[\"company\"])[10])\n",
    "cleaned_df['lemmatized'] = cleaned_df.apply(lambda row: \" \".join(tokenize(row['text'], get_company_names(row[\"company\"]))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Negative', 'Neutral', 'Strongly Positive',\n",
       "       'Strongly Negative'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cleaned_df[\"lemmatized\"])[0]\n",
    "cleaned_df[\"Answer.sentiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation model fit in 16.952 seconds\n",
      "Classification Report:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         Negative       0.45      0.90      0.60        10\n",
      "          Neutral       0.00      0.00      0.00         8\n",
      "         Positive       0.33      0.55      0.41        11\n",
      "Strongly Negative       0.00      0.00      0.00         6\n",
      "Strongly Positive       0.50      0.12      0.20         8\n",
      "\n",
      "      avg / total       0.28      0.37      0.28        43\n",
      "\n",
      "Building complete model and saving ...\n",
      "Complete model fit in 29.649 seconds\n",
      "-4.2145             21    5.0548          shale\n",
      "-4.0779       football    4.5045           file\n",
      "-3.5971            use    4.4127            sea\n",
      "-3.5618      apartment    3.9375            ice\n",
      "-3.3879     management    3.6615       refinery\n",
      "-3.2350       milkweed    3.5665         motion\n",
      "-3.0131       emission    3.4758             .”\n",
      "-2.9990           palm    3.4435        science\n",
      "-2.9845          power    3.4231        lawsuit\n",
      "-2.8489           well    3.4054        climate\n",
      "-2.8455        plastic    3.3935   spokesperson\n",
      "-2.8153       reindeer    3.3140       standard\n",
      "-2.7986           list    3.1807      automaker\n",
      "-2.7563             01    3.1529         diesel\n",
      "-2.7251       refinish    3.0621       security\n",
      "-2.6945         fossil    3.0194            oil\n",
      "-2.6778        project    3.0016        bailout\n",
      "-2.6570              —    2.9407           deal\n",
      "-2.5526         supply    2.9198      defendant\n",
      "-2.5118     production    2.8515           firm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = cleaned_df[\"text\"]\n",
    "y = cleaned_df[\"Answer.sentiment\"]\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42,\n",
    "                                                    stratify=y)\n",
    "model, secs = build_and_evaluate(X,y)\n",
    "print(show_most_informative_features(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation model fit in 18.253 seconds\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.90      0.69        10\n",
      "          1       0.96      0.81      0.88        31\n",
      "          2       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.82      0.79      0.79        43\n",
      "\n",
      "Building complete model and saving ...\n",
      "Complete model fit in 22.287 seconds\n",
      "-4.1960        project    4.8262          shale\n",
      "-3.5618      apartment    4.4291              “\n",
      "-3.3744       football    4.3444       fracking\n",
      "-3.2894           meat    4.2346          tonne\n",
      "-3.2490              —    3.9310            oil\n",
      "-3.2350       milkweed    3.7858             .”\n",
      "-3.1625       hydrogen    3.7112            bee\n",
      "-3.1284     protection    3.6193         diesel\n",
      "-3.1016          speak    3.5783           case\n",
      "-3.0697    application    3.2539            ban\n",
      "-2.8599           also    3.2091      pollution\n",
      "-2.8153       reindeer    3.1147        scandal\n",
      "-2.7251       refinish    3.0960       standard\n",
      "-2.6776     microgrids    3.0790        exclude\n",
      "-2.6654         rubber    3.0016        bailout\n",
      "-2.6608          email    2.9510            ice\n",
      "-2.6528         member    2.9081        climate\n",
      "-2.5922         insect    2.8831        science\n",
      "-2.5789          check    2.8691       emission\n",
      "-2.5503        vehicle    2.7481            sea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmarchenko/projects/prj-nlp/env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = cleaned_df[\"text\"]\n",
    "y = cleaned_df[\"simple_sentiment\"]\n",
    "# test_size = 0.2\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42,\n",
    "#                                                     stratify=y)\n",
    "model, secs = build_and_evaluate(X,y)\n",
    "print(show_most_informative_features(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../1-Data/3-annotation/compiled_output.csv\")\n",
    "df.columns\n",
    "df['text'] = df.apply(lambda row: \"{} {}\".format(row['Input.title'], clean_text(str(row['Input.content']))), axis=1)\n",
    "df.drop(df[df.text.str.len() < 150].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABB: p: 52, n: 2, 0.038\n",
      "AMD (Advanced Micro Devices): p: 53, n: 4, 0.075\n",
      "Amazon.com Inc.: p: 43, n: 4, 0.093\n",
      "Apple Inc: p: 52, n: 9, 0.173\n",
      "AuFeminin.com: p: 45, n: 18, 0.4\n",
      "BASF AG: p: 112, n: 20, 0.179\n",
      "BMW: p: 98, n: 18, 0.184\n",
      "BP plc: p: 13, n: 23, 1.769\n",
      "Bayer AG: p: 41, n: 13, 0.317\n",
      "Bridgestone Corp.: p: 61, n: 5, 0.082\n",
      "Bunge: p: 51, n: 11, 0.216\n",
      "Cargill Inc: p: 35, n: 6, 0.171\n",
      "Chevron Corp.: p: 23, n: 86, 3.739\n",
      "Continental AG: p: 60, n: 13, 0.217\n",
      "Daimler AG: p: 100, n: 20, 0.2\n",
      "Dow Chemical: p: 40, n: 20, 0.5\n",
      "Duke Energy: p: 41, n: 6, 0.146\n",
      "EDS Corp.: p: 29, n: 11, 0.379\n",
      "Exxon Mobil Corp.: p: 9, n: 39, 4.333\n",
      "Ford Motor Co.: p: 87, n: 29, 0.333\n",
      "GazProm: p: 45, n: 21, 0.467\n",
      "General Motors Corp.: p: 115, n: 21, 0.183\n",
      "Goodyear Tire & Rubber Co: p: 46, n: 7, 0.152\n",
      "IBM: p: 55, n: 6, 0.109\n",
      "Ineos: p: 10, n: 40, 4.0\n",
      "Johnson & Johnson: p: 50, n: 21, 0.42\n",
      "Michelin: p: 59, n: 2, 0.034\n",
      "Microsoft Corp.: p: 49, n: 5, 0.102\n",
      "Motorola Inc.: p: 6, n: 3, 0.5\n",
      "Nestle SA: p: 34, n: 6, 0.176\n",
      "PepsiCo Inc.: p: 49, n: 10, 0.204\n",
      "Petro China: p: 61, n: 31, 0.508\n",
      "Petrobras: p: 62, n: 43, 0.694\n",
      "Procter & Gamble, Co.: p: 57, n: 8, 0.14\n",
      "RosNeft: p: 29, n: 67, 2.31\n",
      "Royal Dutch Shell PLC: p: 49, n: 64, 1.306\n",
      "Samsung Electronics Co., Ltd.: p: 42, n: 5, 0.119\n",
      "Sony Corp.: p: 43, n: 3, 0.07\n",
      "Sumitomo Rubber Industries: p: 96, n: 4, 0.042\n",
      "Toshiba Corp.: p: 44, n: 6, 0.136\n",
      "Total SA: p: 39, n: 22, 0.564\n",
      "Toyota Motor Corp.: p: 127, n: 12, 0.094\n",
      "Volkswagen AG: p: 32, n: 18, 0.562\n"
     ]
    }
   ],
   "source": [
    "companies = df.groupby(\"Input.company\", as_index=False)\n",
    "rows_list = []\n",
    "for num, cp in enumerate(list(companies.groups.keys())):\n",
    "    cp_df = df.loc[df['Input.company'] == cp]\n",
    "#     cp_df['lemmatized'] = cp_df.apply(lambda row: \" \".join(tokenize(row['text'], get_company_names(row[\"Input.company\"]))), axis=1)\n",
    "\n",
    "#     cp_df['text'] = cp_df.apply(lambda row: \"{} {}\".format(row['Input.title'], clean_text(str(row['Input.content']))), axis=1)\n",
    "    \n",
    "    res = model.predict(cp_df['text'])\n",
    "    counter = Counter(res)\n",
    "    \n",
    "    if counter[1] > 0:\n",
    "        mark = counter[0]/counter[1]\n",
    "    else:\n",
    "        mark = counter[0]\n",
    "#     import ipdb; ipdb.set_trace()\n",
    "    rows_list.append({\"company\": cp, \"pos\":Counter(res)[1], \"neg\": Counter(res)[0], \"mark\":  mark})\n",
    "    print(f\"{cp}: p: {Counter(res)[1]}, n: {Counter(res)[0]}, {round(mark,3)}\")\n",
    "    \n",
    "final_df =  pd.DataFrame(rows_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 18, 1: 32, 2: 2})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>mark</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Exxon Mobil Corp.</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Ineos</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chevron Corp.</td>\n",
       "      <td>3.739130</td>\n",
       "      <td>86</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>RosNeft</td>\n",
       "      <td>2.310345</td>\n",
       "      <td>67</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BP plc</td>\n",
       "      <td>1.769231</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Royal Dutch Shell PLC</td>\n",
       "      <td>1.306122</td>\n",
       "      <td>64</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Petrobras</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>43</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Total SA</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Volkswagen AG</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Petro China</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>31</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Dow Chemical</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Motorola Inc.</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GazProm</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>21</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Johnson &amp; Johnson</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>21</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AuFeminin.com</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>18</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>EDS Corp.</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ford Motor Co.</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>29</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bayer AG</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>13</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Continental AG</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bunge</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>11</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PepsiCo Inc.</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>10</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Daimler AG</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BMW</td>\n",
       "      <td>0.183673</td>\n",
       "      <td>18</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>General Motors Corp.</td>\n",
       "      <td>0.182609</td>\n",
       "      <td>21</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BASF AG</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>20</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Nestle SA</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>9</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Cargill Inc</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Goodyear Tire &amp; Rubber Co</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Duke Energy</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Procter &amp; Gamble, Co.</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>8</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Toshiba Corp.</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>6</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Samsung Electronics Co., Ltd.</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>IBM</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Microsoft Corp.</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Toyota Motor Corp.</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>12</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon.com Inc.</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bridgestone Corp.</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>5</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMD (Advanced Micro Devices)</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Sony Corp.</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Sumitomo Rubber Industries</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>4</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABB</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Michelin</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          company      mark  neg  pos\n",
       "18              Exxon Mobil Corp.  4.333333   39    9\n",
       "24                          Ineos  4.000000   40   10\n",
       "12                  Chevron Corp.  3.739130   86   23\n",
       "34                        RosNeft  2.310345   67   29\n",
       "7                          BP plc  1.769231   23   13\n",
       "35          Royal Dutch Shell PLC  1.306122   64   49\n",
       "32                      Petrobras  0.693548   43   62\n",
       "40                       Total SA  0.564103   22   39\n",
       "42                  Volkswagen AG  0.562500   18   32\n",
       "31                    Petro China  0.508197   31   61\n",
       "15                   Dow Chemical  0.500000   20   40\n",
       "28                  Motorola Inc.  0.500000    3    6\n",
       "20                        GazProm  0.466667   21   45\n",
       "25              Johnson & Johnson  0.420000   21   50\n",
       "4                   AuFeminin.com  0.400000   18   45\n",
       "17                      EDS Corp.  0.379310   11   29\n",
       "19                 Ford Motor Co.  0.333333   29   87\n",
       "8                        Bayer AG  0.317073   13   41\n",
       "13                 Continental AG  0.216667   13   60\n",
       "10                          Bunge  0.215686   11   51\n",
       "30                   PepsiCo Inc.  0.204082   10   49\n",
       "14                     Daimler AG  0.200000   20  100\n",
       "6                             BMW  0.183673   18   98\n",
       "21           General Motors Corp.  0.182609   21  115\n",
       "5                         BASF AG  0.178571   20  112\n",
       "29                      Nestle SA  0.176471    6   34\n",
       "3                       Apple Inc  0.173077    9   52\n",
       "11                    Cargill Inc  0.171429    6   35\n",
       "22      Goodyear Tire & Rubber Co  0.152174    7   46\n",
       "16                    Duke Energy  0.146341    6   41\n",
       "33          Procter & Gamble, Co.  0.140351    8   57\n",
       "39                  Toshiba Corp.  0.136364    6   44\n",
       "36  Samsung Electronics Co., Ltd.  0.119048    5   42\n",
       "23                            IBM  0.109091    6   55\n",
       "27                Microsoft Corp.  0.102041    5   49\n",
       "41             Toyota Motor Corp.  0.094488   12  127\n",
       "2                 Amazon.com Inc.  0.093023    4   43\n",
       "9               Bridgestone Corp.  0.081967    5   61\n",
       "1    AMD (Advanced Micro Devices)  0.075472    4   53\n",
       "37                     Sony Corp.  0.069767    3   43\n",
       "38     Sumitomo Rubber Industries  0.041667    4   96\n",
       "0                             ABB  0.038462    2   52\n",
       "26                       Michelin  0.033898    2   59"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.sort_values(['mark'], ascending=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print(f\"length before: {len(df)}\")\n",
    "# df['title'] = df['Input.title']\n",
    "# df['company'] = df['Input.company']\n",
    "# df['text'] = df.apply(lambda row: \"{} {}\".format(row['Input.title'], clean_text(str(row['Input.content']))), axis=1)\n",
    "\n",
    "# for num, row in df.iterrows():\n",
    "#     cc = find_company_confidence(row)\n",
    "#     if cc == 0:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['company_confidence'] = df.apply(lambda row: find_company_confidence(row) ,axis=1)\n",
    "# df = df[df['company_confidence'][0] != 0]\n",
    "# print(f\"length after: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['company_confidence'] = df.apply(lambda row: row['company_confidence'][0] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['company_confidence'] != 0]\n",
    "# print(f\"length after: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# companies = df.groupby(\"Input.company\", as_index=False)\n",
    "# rows_list = []\n",
    "# for num, cp in enumerate(list(companies.groups.keys())):\n",
    "#     cp_df = df.loc[df['Input.company'] == cp]\n",
    "#     res = model.predict(cp_df['text'])\n",
    "#     rows_list.append({\"company\": cp, \"pos\":Counter(res)[1], \"neg\": Counter(res)[0], \"mark\": Counter(res)[0]/Counter(res)[1] })\n",
    "#     print(f\"{cp}: p: {Counter(res)[1]}, n: {Counter(res)[0]}, {round(Counter(res)[0]/Counter(res)[1],3)}\")\n",
    "    \n",
    "# final_df =  pd.DataFrame(rows_list) \n",
    "# final_df.sort_values(['mark'], ascending=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "sentiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
