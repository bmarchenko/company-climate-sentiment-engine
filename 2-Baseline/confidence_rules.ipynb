{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from boilerpipe.extract import Extractor\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words())\n",
    "ANNOTATION_RESULTS = \"../1-Data/3-annotation/output.csv\"\n",
    "df = pd.read_csv(ANNOTATION_RESULTS)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "alternative_company_names = {\"AMD (Advanced Micro Devices)\": \"AMD\",\n",
    "                    'Royal Dutch Shell PLC': \"Shell\",\n",
    "                    \"Samsung Electronics Co., Ltd.\": \"Samsung\",\n",
    "                    \"Goodyear Tire & Rubber Co\": \"Goodyear\",\n",
    "                    \"Sumitomo Rubber Industries\": \"Sumitomo\",\n",
    "                    \"Exxon Mobil Corp.\": \"Exxon\",\n",
    "                    \"General Motors Corp.\": \"GM\",\n",
    "                    \"Ford Motor Co.\": \"Ford\",\n",
    "                    \"Toyota Motor Corp.\": \"Toyota\",\n",
    "                    \"Petro China\": \"PetroChina\",\n",
    "                    'Volkswagen AG': \"VW\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\") # create a new bs4 object from the html data loaded\n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "    texts = soup.findAll(text=True)\n",
    "    text = \" \".join(t.strip() for t in texts)\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    return text\n",
    "df['text'] = df.apply(lambda row: \"{} {}\".format(row['title'], clean_text(str(row['full_text']))), axis=1)\n",
    "df.drop(df[df.text.str.len() < 150].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company Confidence Baseline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline is built on frequency of mentions of the company, comparing to other organizations:  \n",
    "2 - Very Confident that the text is about this company. It's mention fraction => 10% and at least 2 mentions.  \n",
    "1 - Moderate. It is mentioned but the fraction < 10%.  \n",
    "0 - Not related to the company. The company is not mentioned in text.  \n",
    "TODO: take in account how many organizations mentioned, how many words in the text etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Answer.CompanyConfidence'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_names(company):\n",
    "    company_names = [re.sub(COMPANY_NAMES_STOP_WORDS, '', company, flags=re.IGNORECASE).strip().lower()]\n",
    "    if company in alternative_company_names:\n",
    "        company_names.append(alternative_company_names[company].lower())\n",
    "    #Company is often mentioned by part of it's name. e.g. \"Royal Dutch Shell\" -> \"Shell\"\n",
    "#     company_names = set([company] + [i for i in company.split() if len(i)>2])\n",
    "    return company_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_company_confidence(row):\n",
    "    company_names = get_company_names(row['company'])\n",
    "    #if company name in title, it's about the company\n",
    "    title_rank = 0\n",
    "    if any([name in str(row['title']).lower() for name in company_names]):\n",
    "        doc = nlp(row['title'])\n",
    "        #if other organizations in title it's 1\n",
    "        title_mentions = len([ent for ent in doc.ents if ent.label_ == \"ORG\"])\n",
    "        if title_mentions == 1:\n",
    "            #assumes only company in title\n",
    "            title_rank = 2\n",
    "        elif title_mentions:\n",
    "            title_rank = 1\n",
    "    text = row['text']\n",
    "    doc = nlp(text)\n",
    "    orgs = [re.sub(COMPANY_NAMES_STOP_WORDS, '', ent.lemma_.strip(), flags=re.IGNORECASE).strip().lower() for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "    orgs_counter = Counter(orgs)\n",
    "    #sometimes spacy does not recognize company name as org, try in other tokens\n",
    "    occurences_company = 0\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.lemma_ in company_names:\n",
    "                occurences_company += 1\n",
    "                break\n",
    "    \n",
    "    occurences_total = len(orgs)\n",
    "\n",
    "    if occurences_total == 0:\n",
    "        fraction = 0\n",
    "    else:\n",
    "        fraction = occurences_company/occurences_total\n",
    "#     if row['url'] == 'https://www.treehugger.com/corporate-responsibility/75-companies-which-backed-global-climate-coalition-lies-about-global-warming.html\t':\n",
    "#         import ipdb; ipdb.set_trace()    \n",
    "    if fraction > 0.1 and occurences_company > 2:\n",
    "        return 2, fraction\n",
    "    if occurences_company > 1:\n",
    "        if title_rank == 2:\n",
    "            return 2, 'in_title'\n",
    "        return 1, fraction\n",
    "    elif title_rank == 1:\n",
    "        return 1, 'in_title'\n",
    "    return 0, fraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_confidence = []\n",
    "company_fraction = []\n",
    "# orgs_lst = []\n",
    "for num, row in df.iterrows():\n",
    "    c_confidence, frac = find_company_confidence(row)\n",
    "    company_confidence.append(c_confidence)\n",
    "    company_fraction.append(frac)\n",
    "\n",
    "df['company_confidence_estimation'] = company_confidence\n",
    "df['company_fraction'] = company_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df['Answer.CompanyConfidence'], df['company_confidence_estimation'], target_names=('0', '1', '2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df1.set_index(['url'], inplace=True)\n",
    "def highlight_different(s):\n",
    "    if s['Answer.CompanyConfidence'] != s['company_confidence_estimation']:\n",
    "        return ['background-color: red' if v == s['company_confidence_estimation'] else '' for v in s ]\n",
    "    return ['' for v in s]\n",
    "df1 = df1[['company', 'Answer.CompanyConfidence', 'company_confidence_estimation', 'company_fraction']]\n",
    "df1.style.apply(highlight_different, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Climate Confidence Baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Answer.ClimateConfidence'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_confidence = []\n",
    "climate_fraction = []\n",
    "KEYWORDS = (\"climate\", \"fossil\", \"renewable\", \"carbon\", \"environment\", \"environmental\", \"warming\", \"sustainability\", \"sustainable\")\n",
    "\n",
    "for num, row in df.iterrows():\n",
    "    in_title = False\n",
    "    if any([name in str(row['title']).lower() for name in KEYWORDS]):\n",
    "        in_title = True\n",
    "        \n",
    "    doc = nlp(row['text'])\n",
    "    tokens = split_into_lemmas(doc)\n",
    "    num = sum([tokens.count(word) for word in KEYWORDS])\n",
    "#     import ipdb; ipdb.set_trace()\n",
    "    fraction = num/len(tokens)\n",
    "    if num >  4:\n",
    "        climate_confidence.append(2)\n",
    "    elif num > 2:\n",
    "        if in_title:\n",
    "            climate_confidence.append(2)\n",
    "            climate_fraction.append('in title')\n",
    "            continue\n",
    "        climate_confidence.append(1)\n",
    "    else:\n",
    "        climate_confidence.append(0)\n",
    "    climate_fraction.append(fraction)\n",
    "df['climate_confidence_estimation'] = climate_confidence\n",
    "df['climate_fraction'] = climate_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df1.set_index(['url'], inplace=True)\n",
    "def highlight_different(s):\n",
    "    if s['Answer.ClimateConfidence'] != s['climate_confidence_estimation']:\n",
    "        return ['background-color: red' if v == s['climate_confidence_estimation'] else '' for v in s ]\n",
    "    return ['' for v in s]\n",
    "df1 = df1[['Answer.ClimateConfidence', 'climate_confidence_estimation', 'climate_fraction']]\n",
    "df1.style.apply(highlight_different, axis=1)\n",
    "# df[['Answer.ClimateConfidence', 'climate_confidence_estimation', 'climate_fraction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text\"]\n",
    "y = df[\"Answer.ClimateConfidence\"]\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "vect = CountVectorizer()\n",
    "cls = SGDClassifier()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('cls',cls),\n",
    "#     ('stop_words', stop)\n",
    "])\n",
    "pipeline.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rule Based\")\n",
    "print(classification_report(df['Answer.ClimateConfidence'], df['climate_confidence_estimation'], target_names=('0', '1', '2')))\n",
    "print(\"BoW\")\n",
    "print(classification_report(y_test, pipeline.predict(X_test), target_names=('0', '1', '2')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "sentiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
